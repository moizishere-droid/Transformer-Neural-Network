ğŸ§  Transformer From Scratch â€” English â†’ Urdu Neural Machine Translation

This repository contains my complete implementation of a **Transformer architecture built entirely from scratch in PyTorch**, applied to **English-to-Urdu translation**.

Unlike most end-to-end tutorials, I developed **each Transformer component in a separate file**, tested them individually, and then combined them into a clean and efficient final model (`transformer.py`).

---

## ğŸ§© Project Overview

I designed and implemented each part of the Transformer manually, without using any high-level APIs (like `nn.Transformer` from PyTorch).  
This helped me deeply understand how **attention, embeddings, and encoder-decoder mechanisms** actually work.

Hereâ€™s how the project evolved:

| Stage | Component | Description |
|--------|------------|-------------|
| 1ï¸âƒ£ | `self_attention.py` | Implemented **Scaled Dot-Product Self-Attention** from scratch. |
| 2ï¸âƒ£ | `multi_head_attention.py` | Extended self-attention to **Multi-Head Attention**, splitting and concatenating heads manually. |
| 3ï¸âƒ£ | `layer_norm.py` | Wrote **Layer Normalization** by hand for stabilizing training. |
| 4ï¸âƒ£ | `embedding.py` | Built **Token + Positional Embedding** logic using word indices and learnable embeddings. |
| 5ï¸âƒ£ | `positional_encoding.py` | Added **sinusoidal positional encoding** to preserve word order in sequences. |
| 6ï¸âƒ£ | `encoder.py` | Combined all components (attention, normalization, feed-forward) to form a **Transformer Encoder block**. |
| 7ï¸âƒ£ | `decoder.py` | Implemented **Masked Decoder** with self-attention, cross-attention, and feed-forward layers. |
| 8ï¸âƒ£ | `transformer.py` | Integrated everything into a final **Transformer model** with clean, modular structure. |
| 9ï¸âƒ£ | `Transformer_Trainer.ipynb` | Trained the model on a bilingual dataset (English â†’ Urdu). |
| ğŸ”Ÿ | Model saved as `transformer_eng_ur_epochX.pt` after training. |

---

## ğŸŒ English â†’ Urdu Translation

The final Transformer was trained on a custom dataset:
- File: `english_to_urdu_dataset.xlsx`
- Columns: `eng`, `urdu`
- Total Clean Pairs: ~9,000

After preprocessing, tokenization, and training, the model learned to translate English sentences into Urdu text.

**Example:**
```

EN: how are you
UR: Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ

```

---

## ğŸ—ï¸ Folder Structure

```

â”œâ”€â”€ self_attention.py
â”œâ”€â”€ multi_head_attention.py
â”œâ”€â”€ layer_norm.py
â”œâ”€â”€ embedding.py
â”œâ”€â”€ positional_encoding.py
â”œâ”€â”€ encoder.py
â”œâ”€â”€ decoder.py
â”œâ”€â”€ transformer.py                # Final clean and complete Transformer
â”œâ”€â”€ Transformer_Trainer.ipynb     # Model training notebook
â”œâ”€â”€ english_to_urdu_dataset.xlsx  # Bilingual dataset
â”œâ”€â”€ transformer_eng_ur_epoch6.pt  # Trained model
â”œâ”€â”€ README.md

````

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/<your-username>/transformer-eng-urdu.git
cd transformer-eng-urdu
pip install -r requirements.txt
````

---

## ğŸ§  Model Details

| Component                     | Description                                            |
| ----------------------------- | ------------------------------------------------------ |
| **Embedding**                 | Converts words into dense vectors                      |
| **Positional Encoding**       | Adds sequence order using sine/cosine patterns         |
| **Self-Attention**            | Learns dependencies within a sentence                  |
| **Multi-Head Attention**      | Allows parallel attention focus                        |
| **Layer Normalization**       | Stabilizes gradients and convergence                   |
| **Encoderâ€“Decoder Structure** | Follows the original *Attention Is All You Need* paper |
| **Loss Function**             | CrossEntropyLoss                                       |
| **Optimizer**                 | Adam                                                   |

---

## ğŸ’¬ Translation Example

```python
from transformer import Transformer
import torch

# Load model
transformer = Transformer(...)
transformer.load_state_dict(torch.load("/content/transformer_eng_ur_epoch6.pt"))
transformer.eval()

# Translate
print(translate_sentence("how are you", transformer, eng_tokenizer, urdu_tokenizer, index_to_urdu, device="cuda"))
```

**Output:**

```
EN: how are you
UR: Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ
```

---

## ğŸ“ˆ Future Enhancements

* [ ] Add Byte Pair Encoding (BPE) for better tokenization
* [ ] Train on a larger parallel corpus
* [ ] Visualize attention weights
* [ ] Deploy model with Streamlit or Gradio web UI

---

## ğŸ‘¨â€ğŸ’» Author

**Abdul Moiz**
ğŸ¯ *Aspiring AI/ML Engineer*
---
