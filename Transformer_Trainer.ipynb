{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformer import Transformer"
      ],
      "metadata": {
        "id": "-U8vCVpc8pTT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXCEL_PATH = \"/content/english_to_urdu_dataset.xlsx\"\n",
        "TOKENIZER_EN_PREFIX = \"/content/tokenizer_en\"   # prefix used when saved\n",
        "TOKENIZER_UR_PREFIX = \"/content/tokenizer_ur\"\n",
        "BUILD_TOKENIZERS_IF_MISSING = True\n",
        "TARGET_VOCAB_SIZE = 2 ** 13\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 40           # must match transformer's max_sequence_length used at init\n",
        "D_MODEL = 256\n",
        "FFN_HIDDEN = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 4\n",
        "DROP_PROB = 0.1\n",
        "LR = 1e-4\n",
        "EPOCHS = 6\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NEG_INF = -1e9"
      ],
      "metadata": {
        "id": "PLH-h4UM8pQ2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(EXCEL_PATH, engine='openpyxl')  # engine explicit for .xlsx\n",
        "\n",
        "# detect likely columns\n",
        "possible_cols = [c.lower() for c in df.columns]\n",
        "if 'english' in possible_cols:\n",
        "    en_col = df.columns[possible_cols.index('english')]\n",
        "elif 'eng' in possible_cols:\n",
        "    en_col = df.columns[possible_cols.index('eng')]\n",
        "elif 'English' in df.columns:\n",
        "    en_col = 'English'\n",
        "else:\n",
        "    # If your excel has other names, update here\n",
        "    en_col = df.columns[0]\n",
        "\n",
        "if 'urdu' in possible_cols:\n",
        "    ur_col = df.columns[possible_cols.index('urdu')]\n",
        "elif 'Urdu' in df.columns:\n",
        "    ur_col = 'Urdu'\n",
        "else:\n",
        "    ur_col = df.columns[1]\n",
        "\n",
        "df = df[[en_col, ur_col]].rename(columns={en_col: 'eng', ur_col: 'urdu'})"
      ],
      "metadata": {
        "id": "J7KCtta-8pOR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning functions\n",
        "def clean_english(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'[^a-z0-9\\s]', '', text.lower()).strip()\n",
        "    return ''\n",
        "\n",
        "def clean_urdu(text):\n",
        "    if isinstance(text, str):\n",
        "        # remove ASCII letters/digits/punctuation; keep unicode (Urdu)\n",
        "        return re.sub(r'[A-Za-z0-9!\"#$%&\\'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~]', '', text).strip()\n",
        "    return ''\n",
        "\n",
        "df['eng'] = df['eng'].apply(clean_english)\n",
        "df['urdu'] = df['urdu'].apply(clean_urdu)\n",
        "df = df[(df['eng'] != '') & (df['urdu'] != '')].reset_index(drop=True)\n",
        "\n",
        "print(\"Total cleaned pairs:\", len(df))"
      ],
      "metadata": {
        "id": "7Ej6Ok0S8pL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f566872-81db-4159-b437-bdd54015368e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cleaned pairs: 9101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "try:\n",
        "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(TOKENIZER_EN_PREFIX)\n",
        "    tokenizer_ur = tfds.deprecated.text.SubwordTextEncoder.load_from_file(TOKENIZER_UR_PREFIX)\n",
        "    print(\"Loaded tokenizers from disk.\")\n",
        "except Exception as e:\n",
        "    print(\"Couldn't load tokenizers from disk:\", e)\n",
        "    if not BUILD_TOKENIZERS_IF_MISSING:\n",
        "        raise RuntimeError(\"Set BUILD_TOKENIZERS_IF_MISSING=True to build tokenizers from the dataset.\")\n",
        "    # Build tokenizers from training corpus (slow)\n",
        "    english_corpus = df['eng'].tolist()\n",
        "    urdu_corpus = df['urdu'].tolist()\n",
        "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(english_corpus, target_vocab_size=TARGET_VOCAB_SIZE)\n",
        "    tokenizer_ur = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(urdu_corpus, target_vocab_size=TARGET_VOCAB_SIZE)\n",
        "    tokenizer_en.save_to_file(TOKENIZER_EN_PREFIX)\n",
        "    tokenizer_ur.save_to_file(TOKENIZER_UR_PREFIX)\n",
        "    print(\"Built & saved tokenizers.\")\n",
        "\n",
        "# Helper: map tokenizer ids -> subword text piece (string)\n",
        "# SubwordTextEncoder uses .subwords list (length = vocab_size)\n",
        "subwords_en = list(tokenizer_en.subwords)\n",
        "subwords_ur = list(tokenizer_ur.subwords)\n",
        "\n",
        "# Add special tokens as textual tokens for your SentenceEmbedding mapping\n",
        "START_TOKEN = \"<S>\"\n",
        "END_TOKEN = \"</S>\"\n",
        "PADDING_TOKEN = \"<PAD>\"\n",
        "\n",
        "# English token strings and mapping to indices (for SentenceEmbedding)\n",
        "english_token_list = subwords_en + [START_TOKEN, END_TOKEN, PADDING_TOKEN]\n",
        "english_to_index = {tok: idx for idx, tok in enumerate(english_token_list)}\n",
        "index_to_english = {idx: tok for tok, idx in english_to_index.items()}\n",
        "\n",
        "# Urdu token strings and mapping to indices\n",
        "urdu_token_list = subwords_ur + [START_TOKEN, END_TOKEN, PADDING_TOKEN]\n",
        "urdu_to_index = {tok: idx for idx, tok in enumerate(urdu_token_list)}\n",
        "index_to_urdu = {idx: tok for tok, idx in urdu_to_index.items()}\n",
        "\n",
        "print(\"English tokens:\", len(english_token_list), \" Urdu tokens:\", len(urdu_token_list))\n",
        "\n",
        "# PAD index used by loss\n",
        "PAD_IDX_URDU = urdu_to_index[PADDING_TOKEN]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWzy_Eu6-Ufi",
        "outputId": "0427fe0d-abe7-458b-db55-58c3393860e4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't load tokenizers from disk: /content/tokenizer_en.subwords; No such file or directory\n",
            "Built & saved tokenizers.\n",
            "English tokens: 7347  Urdu tokens: 8717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EngUrDataset(Dataset):\n",
        "    def __init__(self, eng_texts, ur_texts):\n",
        "        assert len(eng_texts) == len(ur_texts)\n",
        "        self.eng = eng_texts\n",
        "        self.ur = ur_texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_sentence = self.eng[idx]\n",
        "        ur_sentence = self.ur[idx]\n",
        "        # Encode to subword ids (integers)\n",
        "        en_ids = tokenizer_en.encode(en_sentence)\n",
        "        ur_ids = tokenizer_ur.encode(ur_sentence)\n",
        "        # Convert ids to subword strings (these are the tokens our SentenceEmbedding maps)\n",
        "        en_tokens = []\n",
        "        for i in en_ids:\n",
        "            if i < len(subwords_en):\n",
        "                en_tokens.append(subwords_en[i])\n",
        "            elif i == len(subwords_en):  # START token\n",
        "                en_tokens.append(\"<S>\")\n",
        "            elif i == len(subwords_en) + 1:  # END token\n",
        "                en_tokens.append(\"</S>\")\n",
        "\n",
        "        ur_tokens = []\n",
        "        for i in ur_ids:\n",
        "            if i < len(subwords_ur):\n",
        "                ur_tokens.append(subwords_ur[i])\n",
        "            elif i == len(subwords_ur):  # START token\n",
        "                ur_tokens.append(\"<S>\")\n",
        "            elif i == len(subwords_ur) + 1:  # END token\n",
        "                ur_tokens.append(\"</S>\")\n",
        "\n",
        "        return en_tokens, ur_tokens\n",
        "\n",
        "def collate_batch(batch):\n",
        "    # returns lists of token-lists\n",
        "    en_batch, ur_batch = zip(*batch)\n",
        "    return list(en_batch), list(ur_batch)\n",
        "\n",
        "# Train/val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts = train_test_split(df, test_size=0.1, random_state=42)\n",
        "train_ds = EngUrDataset(train_texts['eng'].tolist(), train_texts['urdu'].tolist())\n",
        "val_ds = EngUrDataset(val_texts['eng'].tolist(), val_texts['urdu'].tolist())\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "lzw2nXxL-nBD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(eng_batch, ur_batch, max_sequence_length):\n",
        "    \"\"\"\n",
        "    eng_batch, ur_batch: lists of token-lists (e.g. [ ['this','is','a'], ... ])\n",
        "    returns encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
        "    Each mask will be a float tensor with NEG_INF for masked positions and 0 for allowed.\n",
        "    Shapes: (batch, max_seq, max_seq)\n",
        "    \"\"\"\n",
        "    batch_size = len(eng_batch)\n",
        "    look_ahead_mask = torch.triu(torch.ones((max_sequence_length, max_sequence_length), dtype=torch.bool), diagonal=1)\n",
        "    # same shape as earlier\n",
        "    encoder_padding_mask = torch.zeros((batch_size, max_sequence_length, max_sequence_length), dtype=torch.bool)\n",
        "    decoder_padding_mask_self = torch.zeros((batch_size, max_sequence_length, max_sequence_length), dtype=torch.bool)\n",
        "    decoder_padding_mask_cross = torch.zeros((batch_size, max_sequence_length, max_sequence_length), dtype=torch.bool)\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        eng_len = len(eng_batch[idx]) + 1 + 1  # start + tokens + end -> SentenceEmbedding will add start & end\n",
        "        ur_len = len(ur_batch[idx]) + 1 + 1\n",
        "\n",
        "        # indices that will be padding (positions after actual length)\n",
        "        eng_pad_positions = np.arange(eng_len, max_sequence_length)\n",
        "        ur_pad_positions = np.arange(ur_len, max_sequence_length)\n",
        "\n",
        "        # For encoder padding mask: mask rows/cols corresponding to padding tokens.\n",
        "        encoder_padding_mask[idx, :, eng_pad_positions] = True\n",
        "        encoder_padding_mask[idx, eng_pad_positions, :] = True\n",
        "\n",
        "        # Decoder self-attention padding mask (prevent padding tokens from attending and being attended to)\n",
        "        decoder_padding_mask_self[idx, :, ur_pad_positions] = True\n",
        "        decoder_padding_mask_self[idx, ur_pad_positions, :] = True\n",
        "\n",
        "        # Cross-attention: prevent decoder positions (including padded) attending to encoder padding\n",
        "        decoder_padding_mask_cross[idx, :, eng_pad_positions] = True\n",
        "        decoder_padding_mask_cross[idx, ur_pad_positions, :] = True\n",
        "\n",
        "    # convert to float masks with NEG_INF for masked positions (matching your scaling usage)\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "    # combine lookahead + decoder padding for decoder self-attention\n",
        "    # look_ahead_mask has shape (max_seq, max_seq) boolean; expand to batch\n",
        "    look_ahead_mask_bool = look_ahead_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "    decoder_self_attention_mask = torch.where(look_ahead_mask_bool | decoder_padding_mask_self,\n",
        "                                             torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross, torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "\n",
        "    return encoder_self_attention_mask.to(DEVICE), decoder_self_attention_mask.to(DEVICE), decoder_cross_attention_mask.to(DEVICE)"
      ],
      "metadata": {
        "id": "KJJA9ia_-4vd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size_for_model = len(english_token_list)   # equals embedding vocab inside SentenceEmbedding\n",
        "tgt_vocab_size_for_model = len(urdu_token_list)\n",
        "\n",
        "model = Transformer(\n",
        "    d_model=D_MODEL,\n",
        "    ffn_hidden=FFN_HIDDEN,\n",
        "    num_heads=NUM_HEADS,\n",
        "    drop_prob=DROP_PROB,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    max_sequence_length=MAX_SEQ_LEN,\n",
        "    kn_vocab_size=tgt_vocab_size_for_model,\n",
        "    english_to_index=english_to_index,\n",
        "    urdu_to_index=urdu_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ").to(DEVICE)\n",
        "\n",
        "# loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_URDU, reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "LLp8fAof-8tt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    start = time.time()\n",
        "\n",
        "    for batch_idx, (eng_batch, ur_batch) in enumerate(loader):\n",
        "        # eng_batch, ur_batch are lists of token-lists (strings), length = batch_size\n",
        "        batch_size = len(eng_batch)\n",
        "\n",
        "        # Create masks: these assume SentenceEmbedding will add start/end tokens\n",
        "        enc_mask, dec_self_mask, dec_cross_mask = create_masks(eng_batch, ur_batch, MAX_SEQ_LEN)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = model(\n",
        "            x=eng_batch,\n",
        "            y=ur_batch,\n",
        "            encoder_self_attention_mask=enc_mask,\n",
        "            decoder_self_attention_mask=dec_self_mask,\n",
        "            decoder_cross_attention_mask=dec_cross_mask,\n",
        "            enc_start_token=True,\n",
        "            enc_end_token=True,\n",
        "            dec_start_token=True,\n",
        "            dec_end_token=True\n",
        "        )  # preds shape: (batch, seq_len, vocab_size) as floats (logits)\n",
        "\n",
        "        # Build target labels: we need indices (ints) of the target tokens after SentenceEmbedding tokenization\n",
        "        # We'll reuse the SentenceEmbedding logic: sequence length will equal MAX_SEQ_LEN inside model outputs.\n",
        "        # But to compute loss we must get the integer labels tensor:\n",
        "        # We'll convert ur_batch token-lists -> tensor of indices using urdu_to_index, letting SentenceEmbedding's padding match.\n",
        "        def batch_to_indices(batch_tokens, mapping):\n",
        "            # batch_tokens: list of token-lists (strings) for each example (no start/end)\n",
        "            # mapping maps token-string -> int index in embedding\n",
        "            res = []\n",
        "            for tokens in batch_tokens:\n",
        "                idxs = [mapping[t] for t in tokens]               # tokens only (SentenceEmbedding will add start/end)\n",
        "                # SentenceEmbedding.batch_tokenize will insert start & end if flags true, then pad to max_sequence_length\n",
        "                # But here we must mimic exactly what SentenceEmbedding produces:\n",
        "                seq = []\n",
        "                seq.append(mapping[START_TOKEN])\n",
        "                seq.extend(idxs)\n",
        "                seq.append(mapping[END_TOKEN])\n",
        "                # pad\n",
        "                while len(seq) < MAX_SEQ_LEN:\n",
        "                    seq.append(mapping[PADDING_TOKEN])\n",
        "                # truncate if > MAX_SEQ_LEN\n",
        "                seq = seq[:MAX_SEQ_LEN]\n",
        "                res.append(seq)\n",
        "            return torch.tensor(res, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "        target_indices = batch_to_indices(ur_batch, urdu_to_index)  # shape (batch, MAX_SEQ_LEN)\n",
        "\n",
        "        # preds: (batch, seq_len, vocab)\n",
        "        # We will compute loss on all token positions, but exclude PAD positions\n",
        "        preds_flat = preds.view(-1, preds.size(-1))\n",
        "        labels_flat = target_indices.view(-1)\n",
        "\n",
        "        # compute per-token loss with reduction='sum' then divide by actual tokens\n",
        "        loss_sum = criterion(preds_flat, labels_flat)\n",
        "        non_pad = (labels_flat != PAD_IDX_URDU).sum().item()\n",
        "        loss = loss_sum / max(1, non_pad)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss_sum.item()\n",
        "        total_tokens += non_pad\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch {epoch} Batch {batch_idx} Loss per token: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = total_loss / max(1, total_tokens)\n",
        "    print(f\"Epoch {epoch} finished in {time.time()-start:.1f}s, avg loss per token: {epoch_loss:.4f}\")\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "nOx5pTK-_Aps"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, eng_sentence, max_length=MAX_SEQ_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # prepare english as token-list (no start/end)\n",
        "        en_ids = tokenizer_en.encode(eng_sentence)\n",
        "        en_tokens = [subwords_en[i] for i in en_ids]\n",
        "        # initial ur sentence is empty string (model will add start token if dec_start_token=True)\n",
        "        ur_sentence = (\"\",)   # as tuple of one string like your earlier code used\n",
        "        for step in range(max_length):\n",
        "            enc_mask, dec_mask, cross_mask = create_masks([en_tokens], [[]], max_length)\n",
        "            preds = model(\n",
        "                x=[en_tokens],\n",
        "                y=ur_sentence,   # tuple/list; SentenceEmbedding expects sequences; pass as tuple of 1 element\n",
        "                encoder_self_attention_mask=enc_mask,\n",
        "                decoder_self_attention_mask=dec_mask,\n",
        "                decoder_cross_attention_mask=cross_mask,\n",
        "                enc_start_token=True,\n",
        "                enc_end_token=True,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=False\n",
        "            )  # outputs shape (1, max_len, vocab)\n",
        "            # take next token at current step\n",
        "            logits_at_step = preds[0, step]   # (vocab,)\n",
        "            next_idx = torch.argmax(logits_at_step).item()\n",
        "            next_token_str = index_to_urdu[next_idx]\n",
        "            # stop if END_TOKEN\n",
        "            if next_token_str == END_TOKEN:\n",
        "                break\n",
        "            # append token (SentenceEmbedding expects token-lists; we can append string to the single string tuple)\n",
        "            ur_sentence = (ur_sentence[0] + next_token_str, )  # keep format consistent with encoder code\n",
        "        # ur_sentence is a tuple with concatenated token-strings — convert token-strings back to string using tokenizer_ur.decode\n",
        "        # However we constructed tokens as raw subword string tokens so it's tricky to decode; instead, build list of subword tokens:\n",
        "        # We will extract tokens from the built ur_sentence[0] by splitting on token boundaries isn't trivial.\n",
        "        # Simpler: we can re-run generation collecting token strings in a list instead:\n",
        "    return None  # We will implement improved inference below"
      ],
      "metadata": {
        "id": "hCfXWhIP_GOQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better greedy inference that accumulates tokens in list form:\n",
        "def translate_greedy(model, eng_sentence, max_length=MAX_SEQ_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        en_ids = tokenizer_en.encode(eng_sentence)\n",
        "        en_tokens = [subwords_en[i] for i in en_ids]\n",
        "        ur_tokens = []  # list of generated subword tokens (strings), not joined text\n",
        "        for step in range(max_length):\n",
        "            # pass current ur_tokens as batch of one example (note: pass as list-of-strings where each example is list tokens OR empty string)\n",
        "            enc_mask, dec_mask, cross_mask = create_masks([en_tokens], [ur_tokens], max_length)\n",
        "            # model expects batch of token-lists. For target we pass a tuple/list with one element that is a list of token strings.\n",
        "            preds = model(\n",
        "                x=[en_tokens],\n",
        "                y=[ur_tokens],   # batch of one, each is a list of token strings\n",
        "                encoder_self_attention_mask=enc_mask,\n",
        "                decoder_self_attention_mask=dec_mask,\n",
        "                decoder_cross_attention_mask=cross_mask,\n",
        "                enc_start_token=True,\n",
        "                enc_end_token=True,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=False\n",
        "            )  # shape (1, max_len, vocab)\n",
        "            logits = preds[0, len(ur_tokens)]   # logits for next position\n",
        "            next_idx = torch.argmax(logits).item()\n",
        "            next_token = index_to_urdu[next_idx]\n",
        "            if next_token == END_TOKEN:\n",
        "                break\n",
        "            if next_token == PADDING_TOKEN:\n",
        "                break\n",
        "            ur_tokens.append(next_token)\n",
        "        # Now ur_tokens is a list of subword token-strings; we need to convert to text\n",
        "        # Find id of each subword token in tokenizer_ur.subwords and then decode\n",
        "        ur_ids = []\n",
        "        for t in ur_tokens:\n",
        "            if t in urdu_to_index and urdu_to_index[t] < tokenizer_ur.vocab_size:\n",
        "                ur_ids.append(urdu_to_index[t])\n",
        "            else:\n",
        "                # token might be special; skip\n",
        "                pass\n",
        "        # decode to string with tokenizer\n",
        "        ur_text = tokenizer_ur.decode(ur_ids)\n",
        "        return ur_text"
      ],
      "metadata": {
        "id": "7dqI_crG_JXY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, epoch)\n",
        "    # (optional) quick val: translate a few samples\n",
        "    sample_src = val_texts['eng'].iloc[:3].tolist()\n",
        "    for s in sample_src:\n",
        "        translation = translate_greedy(model, s, max_length=MAX_SEQ_LEN)\n",
        "        print(\"SRC:\", s)\n",
        "        print(\"PRED:\", translation)\n",
        "    # you can also save checkpoint\n",
        "    torch.save(model.state_dict(), f\"/content/transformer_eng_ur_epoch{epoch}.pt\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "ytvisBWf_NCr",
        "outputId": "a99214d8-343a-424d-b1ed-ecb2b82fdadd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [40] at entry 0 and [43] at entry 22",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-452309418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# (optional) quick val: translate a few samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msample_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_src\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3085441631.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         preds = model(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meng_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mur_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mdec_start_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# We should make this true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 dec_end_token=False): # x, y are batch of sentences\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mbatch_tokenize\u001b[0;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m            \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [40] at entry 0 and [43] at entry 22"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full corrected training cell for English -> Urdu (PyTorch)\n",
        "import re, time, math\n",
        "import torch, numpy as np, pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----- Hyperparams & paths -----\n",
        "EXCEL_PATH = \"/content/english_to_urdu_dataset.xlsx\"\n",
        "TOKENIZER_EN_PREFIX = \"/content/tokenizer_en\"\n",
        "TOKENIZER_UR_PREFIX = \"/content/tokenizer_ur\"\n",
        "BUILD_TOKENIZERS_IF_MISSING = True\n",
        "TARGET_VOCAB_SIZE = 2**13\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 40          # must match transformer's max_sequence_length\n",
        "D_MODEL = 256\n",
        "FFN_HIDDEN = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 4\n",
        "DROP_PROB = 0.1\n",
        "LR = 1e-4\n",
        "EPOCHS = 6\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NEG_INF = -1e9\n",
        "\n",
        "# ----- Load transformer class -----\n",
        "from transformer import Transformer\n",
        "\n",
        "# ----- Load & clean dataset -----\n",
        "df = pd.read_excel(EXCEL_PATH, engine='openpyxl')\n",
        "# If your columns are already 'eng' and 'urdu' this still works\n",
        "if 'eng' not in df.columns or 'urdu' not in df.columns:\n",
        "    df = df.rename(columns={'English': 'eng', 'Urdu': 'urdu'})\n",
        "\n",
        "def clean_english(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'[^a-z0-9\\s]', '', text.lower()).strip()\n",
        "    return ''\n",
        "\n",
        "def clean_urdu(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'[A-Za-z0-9!\"#$%&\\'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~]', '', text).strip()\n",
        "    return ''\n",
        "\n",
        "df['eng'] = df['eng'].apply(clean_english)\n",
        "df['urdu'] = df['urdu'].apply(clean_urdu)\n",
        "df = df[(df['eng'] != '') & (df['urdu'] != '')].reset_index(drop=True)\n",
        "print(\"✅ Total cleaned pairs:\", len(df))\n",
        "\n",
        "# ----- Train / val split (reset indices) -----\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "print(\"Training pairs:\", len(train_df), \"Validation pairs:\", len(val_df))\n",
        "\n",
        "# ----- Build / Load tokenizers -----\n",
        "!pip install -q tensorflow_datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "try:\n",
        "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(TOKENIZER_EN_PREFIX)\n",
        "    tokenizer_ur = tfds.deprecated.text.SubwordTextEncoder.load_from_file(TOKENIZER_UR_PREFIX)\n",
        "    print(\"✅ Loaded tokenizers from disk.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠ Couldn't load tokenizers:\", e)\n",
        "    if not BUILD_TOKENIZERS_IF_MISSING:\n",
        "        raise RuntimeError(\"Set BUILD_TOKENIZERS_IF_MISSING=True to build tokenizers.\")\n",
        "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "        train_df['eng'].tolist(), target_vocab_size=TARGET_VOCAB_SIZE)\n",
        "    tokenizer_ur = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "        train_df['urdu'].tolist(), target_vocab_size=TARGET_VOCAB_SIZE)\n",
        "    tokenizer_en.save_to_file(TOKENIZER_EN_PREFIX)\n",
        "    tokenizer_ur.save_to_file(TOKENIZER_UR_PREFIX)\n",
        "    print(\"✅ Built & saved tokenizers.\")\n",
        "\n",
        "subwords_en = list(tokenizer_en.subwords)   # length == tokenizer_en.vocab_size\n",
        "subwords_ur = list(tokenizer_ur.subwords)\n",
        "\n",
        "# special tokens used in SentenceEmbedding mapping (strings)\n",
        "START_TOKEN = \"<S>\"\n",
        "END_TOKEN = \"</S>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "# build token->index maps that match your transformer's SentenceEmbedding\n",
        "english_token_list = subwords_en + [START_TOKEN, END_TOKEN, PAD_TOKEN]\n",
        "urdu_token_list = subwords_ur + [START_TOKEN, END_TOKEN, PAD_TOKEN]\n",
        "english_to_index = {t: i for i, t in enumerate(english_token_list)}\n",
        "urdu_to_index  = {t: i for i, t in enumerate(urdu_token_list)}\n",
        "index_to_urdu  = {i: t for t, i in urdu_to_index.items()}\n",
        "\n",
        "PAD_IDX_URDU = urdu_to_index[PAD_TOKEN]\n",
        "\n",
        "print(\"✅ English tokens:\", len(english_token_list), \"Urdu tokens:\", len(urdu_token_list))\n",
        "\n",
        "# ----- Dataset (returns lists of subword strings) -----\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer_en, tokenizer_ur, subwords_en, subwords_ur, max_len=MAX_SEQ_LEN):\n",
        "        self.df = df.reset_index(drop=True)  # safe .iloc usage\n",
        "        self.tokenizer_en = tokenizer_en\n",
        "        self.tokenizer_ur = tokenizer_ur\n",
        "        self.subwords_en = subwords_en\n",
        "        self.subwords_ur = subwords_ur\n",
        "        # reserve 2 slots for start/end tokens when passing into model's SentenceEmbedding\n",
        "        self.max_tokens = max_len - 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        en_text = str(row['eng'])\n",
        "        ur_text = str(row['urdu'])\n",
        "\n",
        "        en_ids = self.tokenizer_en.encode(en_text)\n",
        "        ur_ids = self.tokenizer_ur.encode(ur_text)\n",
        "\n",
        "        # truncate IDs so after adding start/end they fit into MAX_SEQ_LEN\n",
        "        en_ids = en_ids[:self.max_tokens]\n",
        "        ur_ids = ur_ids[:self.max_tokens]\n",
        "\n",
        "        # map ids -> subword strings, ignore any reserved indices (>= vocab_size)\n",
        "        en_tokens = []\n",
        "        for i in en_ids:\n",
        "            if i < len(self.subwords_en):\n",
        "                en_tokens.append(self.subwords_en[i])\n",
        "            # else ignore (we won't include start/end here)\n",
        "        ur_tokens = []\n",
        "        for i in ur_ids:\n",
        "            if i < len(self.subwords_ur):\n",
        "                ur_tokens.append(self.subwords_ur[i])\n",
        "\n",
        "        return en_tokens, ur_tokens\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch is list of (en_tokens, ur_tokens)\n",
        "    en_batch, ur_batch = zip(*batch)\n",
        "    return list(en_batch), list(ur_batch)\n",
        "\n",
        "# ----- DataLoaders -----\n",
        "train_dataset = TranslationDataset(train_df, tokenizer_en, tokenizer_ur, subwords_en, subwords_ur, max_len=MAX_SEQ_LEN)\n",
        "val_dataset   = TranslationDataset(val_df,   tokenizer_en, tokenizer_ur, subwords_en, subwords_ur, max_len=MAX_SEQ_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ----- Mask creation (returns float masks with NEG_INF where masked) -----\n",
        "def create_masks(eng_batch, ur_batch, max_len):\n",
        "    batch_size = len(eng_batch)\n",
        "    look_ahead = torch.triu(torch.ones((max_len, max_len), dtype=torch.bool), diagonal=1)\n",
        "    enc_pad = torch.zeros((batch_size, max_len, max_len), dtype=torch.bool)\n",
        "    dec_pad_self = torch.zeros_like(enc_pad)\n",
        "    dec_pad_cross = torch.zeros_like(enc_pad)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        e_len = len(eng_batch[i]) + 2   # +start +end\n",
        "        u_len = len(ur_batch[i]) + 2\n",
        "        e_pad = np.arange(e_len, max_len)\n",
        "        u_pad = np.arange(u_len, max_len)\n",
        "        if len(e_pad) > 0:\n",
        "            enc_pad[i, :, e_pad] = enc_pad[i, e_pad, :] = True\n",
        "        if len(u_pad) > 0:\n",
        "            dec_pad_self[i, :, u_pad] = dec_pad_self[i, u_pad, :] = True\n",
        "            dec_pad_cross[i, u_pad, :] = True\n",
        "            dec_pad_cross[i, :, e_pad] = enc_pad[i, :, e_pad]  # keep cross consistent\n",
        "\n",
        "    enc_mask = torch.where(enc_pad, torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "    look_ahead = look_ahead.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "    dec_self = torch.where(look_ahead | dec_pad_self, torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "    dec_cross = torch.where(dec_pad_cross, torch.tensor(NEG_INF), torch.tensor(0.0))\n",
        "    return enc_mask.to(DEVICE), dec_self.to(DEVICE), dec_cross.to(DEVICE)\n",
        "\n",
        "# ----- Instantiate model -----\n",
        "model = Transformer(\n",
        "    d_model=D_MODEL,\n",
        "    ffn_hidden=FFN_HIDDEN,\n",
        "    num_heads=NUM_HEADS,\n",
        "    drop_prob=DROP_PROB,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    max_sequence_length=MAX_SEQ_LEN,\n",
        "    kn_vocab_size=len(urdu_token_list),\n",
        "    english_to_index=english_to_index,\n",
        "    urdu_to_index=urdu_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PAD_TOKEN\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_URDU, reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ----- helper to convert batch token-strings -> indices according to urdu_to_index (used for loss) -----\n",
        "def batch_to_indices(batch_tokens, mapping, max_len=MAX_SEQ_LEN):\n",
        "    res = []\n",
        "    for tokens in batch_tokens:\n",
        "        seq = [mapping[START_TOKEN]] + [mapping.get(t, mapping[PAD_TOKEN]) for t in tokens] + [mapping[END_TOKEN]]\n",
        "        seq = seq[:max_len] + [mapping[PAD_TOKEN]] * max(0, max_len - len(seq))\n",
        "        res.append(seq)\n",
        "    return torch.tensor(res, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "# ----- Training step -----\n",
        "def train_one_epoch(model, loader, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    t0 = time.time()\n",
        "    for batch_idx, (eng_batch, ur_batch) in enumerate(loader):\n",
        "        enc_mask, dec_self_mask, dec_cross_mask = create_masks(eng_batch, ur_batch, MAX_SEQ_LEN)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(\n",
        "            x=eng_batch, y=ur_batch,\n",
        "            encoder_self_attention_mask=enc_mask,\n",
        "            decoder_self_attention_mask=dec_self_mask,\n",
        "            decoder_cross_attention_mask=dec_cross_mask,\n",
        "            enc_start_token=True, enc_end_token=True,\n",
        "            dec_start_token=True, dec_end_token=True\n",
        "        )  # (batch, seq_len, vocab)\n",
        "\n",
        "        target_indices = batch_to_indices(ur_batch, urdu_to_index, max_len=MAX_SEQ_LEN)\n",
        "        preds_flat = preds.view(-1, preds.size(-1))\n",
        "        labels_flat = target_indices.view(-1)\n",
        "        loss_sum = criterion(preds_flat, labels_flat)\n",
        "        non_pad = (labels_flat != PAD_IDX_URDU).sum().item()\n",
        "        loss = loss_sum / max(1, non_pad)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss_sum.item()\n",
        "        total_tokens += non_pad\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch {epoch} Batch {batch_idx} Loss/token {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_tokens)\n",
        "    print(f\"Epoch {epoch} finished in {time.time()-t0:.1f}s | Avg loss/token: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "# ----- Greedy translate -----\n",
        "def translate_greedy(model, eng_sentence, max_length=MAX_SEQ_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        en_ids = tokenizer_en.encode(eng_sentence)[:(max_length-2)]\n",
        "        en_tokens = [subwords_en[i] for i in en_ids if i < len(subwords_en)]\n",
        "        ur_tokens = []\n",
        "        for step in range(max_length):\n",
        "            enc_mask, dec_mask, cross_mask = create_masks([en_tokens], [ur_tokens], max_length)\n",
        "            preds = model(\n",
        "                x=[en_tokens], y=[ur_tokens],\n",
        "                encoder_self_attention_mask=enc_mask,\n",
        "                decoder_self_attention_mask=dec_mask,\n",
        "                decoder_cross_attention_mask=cross_mask,\n",
        "                enc_start_token=True, enc_end_token=True,\n",
        "                dec_start_token=True, dec_end_token=False\n",
        "            )\n",
        "            logits = preds[0, len(ur_tokens)]\n",
        "            next_idx = torch.argmax(logits).item()\n",
        "            next_token = index_to_urdu.get(next_idx, PAD_TOKEN)\n",
        "            if next_token in {END_TOKEN, PAD_TOKEN}:\n",
        "                break\n",
        "            ur_tokens.append(next_token)\n",
        "        # convert ur_tokens (subword strings) back to ids for tokenizer decode\n",
        "        ur_ids = [urdu_to_index[t] for t in ur_tokens if t in urdu_to_index and urdu_to_index[t] < tokenizer_ur.vocab_size]\n",
        "        return tokenizer_ur.decode(ur_ids)\n",
        "\n",
        "# ----- Train loop -----\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_loss = train_one_epoch(model, train_loader, epoch)\n",
        "    print(\"\\nSample translations from validation set:\")\n",
        "    for s in val_df['eng'].iloc[:3].tolist():\n",
        "        print(\"EN:\", s)\n",
        "        print(\"PRED UR:\", translate_greedy(model, s))\n",
        "    torch.save(model.state_dict(), f\"/content/transformer_eng_ur_epoch{epoch}.pt\")\n",
        "\n",
        "print(\"🎉 Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq0vRIGSA0dW",
        "outputId": "6045350a-ee6c-427f-aeee-2371b9cfceee"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total cleaned pairs: 9101\n",
            "Training pairs: 8190 Validation pairs: 911\n",
            "✅ Loaded tokenizers from disk.\n",
            "✅ English tokens: 7048 Urdu tokens: 8354\n",
            "Epoch 1 Batch 0 Loss/token 9.0867\n",
            "Epoch 1 Batch 50 Loss/token 5.7523\n",
            "Epoch 1 Batch 100 Loss/token 4.0772\n",
            "Epoch 1 Batch 150 Loss/token 3.3466\n",
            "Epoch 1 Batch 200 Loss/token 2.7486\n",
            "Epoch 1 Batch 250 Loss/token 2.6196\n",
            "Epoch 1 finished in 15.5s | Avg loss/token: 4.2577\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "Epoch 2 Batch 0 Loss/token 2.3217\n",
            "Epoch 2 Batch 50 Loss/token 2.1309\n",
            "Epoch 2 Batch 100 Loss/token 2.2681\n",
            "Epoch 2 Batch 150 Loss/token 1.7576\n",
            "Epoch 2 Batch 200 Loss/token 1.5287\n",
            "Epoch 2 Batch 250 Loss/token 1.5824\n",
            "Epoch 2 finished in 14.6s | Avg loss/token: 1.9183\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "Epoch 3 Batch 0 Loss/token 1.5447\n",
            "Epoch 3 Batch 50 Loss/token 1.3395\n",
            "Epoch 3 Batch 100 Loss/token 1.4177\n",
            "Epoch 3 Batch 150 Loss/token 1.1828\n",
            "Epoch 3 Batch 200 Loss/token 1.2843\n",
            "Epoch 3 Batch 250 Loss/token 1.1532\n",
            "Epoch 3 finished in 15.2s | Avg loss/token: 1.2901\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "Epoch 4 Batch 0 Loss/token 1.0637\n",
            "Epoch 4 Batch 50 Loss/token 0.9936\n",
            "Epoch 4 Batch 100 Loss/token 1.0235\n",
            "Epoch 4 Batch 150 Loss/token 0.9102\n",
            "Epoch 4 Batch 200 Loss/token 0.9095\n",
            "Epoch 4 Batch 250 Loss/token 0.9308\n",
            "Epoch 4 finished in 14.4s | Avg loss/token: 0.9622\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "Epoch 5 Batch 0 Loss/token 0.9002\n",
            "Epoch 5 Batch 50 Loss/token 0.6613\n",
            "Epoch 5 Batch 100 Loss/token 0.6373\n",
            "Epoch 5 Batch 150 Loss/token 0.7036\n",
            "Epoch 5 Batch 200 Loss/token 0.6297\n",
            "Epoch 5 Batch 250 Loss/token 0.6378\n",
            "Epoch 5 finished in 14.5s | Avg loss/token: 0.7557\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "Epoch 6 Batch 0 Loss/token 0.5701\n",
            "Epoch 6 Batch 50 Loss/token 0.6960\n",
            "Epoch 6 Batch 100 Loss/token 0.4974\n",
            "Epoch 6 Batch 150 Loss/token 0.5647\n",
            "Epoch 6 Batch 200 Loss/token 0.5680\n",
            "Epoch 6 Batch 250 Loss/token 0.5179\n",
            "Epoch 6 finished in 14.5s | Avg loss/token: 0.6105\n",
            "\n",
            "Sample translations from validation set:\n",
            "EN: for this cause many are weak and sickly among you  and many sleep\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: toms mad\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "EN: honour widows that are widows indeed\n",
            "PRED UR:  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ،  ، \n",
            "🎉 Training finished.\n"
          ]
        }
      ]
    }
  ]
}